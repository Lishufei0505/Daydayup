{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237ec5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.0a0\n",
      "/home/guest1/LISHUFEI/jupyter_code/yolov5-rt-stack/yolort/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import yolort\n",
    "print(yolort.__version__)\n",
    "print(yolort.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0449d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import platform\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3333052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c0f5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guest1/LISHUFEI/jupyter_code/yolov5-rt-stack\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "FILE = Path().resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOrt root directory\n",
    "print(str(ROOT))\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "print(str(ROOT)in sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdff03",
   "metadata": {},
   "source": [
    "### 配置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09cb02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # PCI_BUS_ID” # 按照PCI_BUS_ID顺序从0开始排列GPU设备 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  #设置当前使用的GPU设备仅为0号设备  设备名称为'/gpu:0'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cuda = device.type != 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0368b",
   "metadata": {},
   "source": [
    "### 导入模型和定义的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3137e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolort.models.yolo import YOLO\n",
    "from yolort.utils import Visualizer, get_image_from_url, read_image_to_tensor, check_dataset\n",
    "from yolort.v5 import load_yolov5_model, letterbox, non_max_suppression, scale_coords, attempt_download\n",
    "from yolort.v5.utils.downloads import safe_download\n",
    "from yolort.v5.utils.dataloaders import *\n",
    "from yolort.v5.utils.general import colorstr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec98c28",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef4c3cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = ROOT / 'data/coco128.yaml'  # dataset.yaml path\n",
    "data = check_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a2777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa4489a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_source = \"https://huggingface.co/spaces/zhiqwang/assets/resolve/main/bus.jpg\"\n",
    "# # img_source = \"https://huggingface.co/spaces/zhiqwang/assets/resolve/main/zidane.jpg\"\n",
    "# img_raw = get_image_from_url(img_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c36f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5693acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = letterbox(img_raw, new_shape=(img_size, img_size), stride=stride)[0]\n",
    "# image = read_image_to_tensor(image)\n",
    "# image = image.to(device)\n",
    "# image = image[None]\n",
    "# print(image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35a12a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 640\n",
    "stride = 64\n",
    "score_thresh = 0.25\n",
    "batch_size = 32\n",
    "nms_thresh = 0.45\n",
    "single_cls=False # treat as single-class dataset\n",
    "rect = False\n",
    "workers=8  # max dataloader workers (per RANK in DDP mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be247188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "895cb217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mScanning '/home/guest1/LISHUFEI/jupyter_code/datasets/coco128/labels/train2017.cache' images and labels... 128 found, 0 missing, 2 empty, \u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#images_source = \n",
    "#我现在需要做的是不是把images/train2017的图片加载进来？\n",
    "task = 'val'\n",
    "task = task if task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n",
    "loader,dataset = create_dataloader(data[task],\n",
    "                               img_size,\n",
    "                               stride,\n",
    "                               batch_size,\n",
    "                               single_cls,\n",
    "                               rect=rect,\n",
    "                               workers=workers,\n",
    "                               prefix=colorstr(f'{task}: '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d950ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(loader)) #loader 的长度是4 128/32\n",
    "# print(len(dataset))  # 数据集的长度是128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e64a7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create_dataloader的返回值是什么？\n",
    "# print(len(loader))  # batch_size num = 4\n",
    "# ime = dataset[0] # 读取到的图像是一个元组类型的  ime[2]\n",
    "# print(dataset[0])\n",
    "# print(ime[2])\n",
    "# print(type(ime[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d7809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25726851",
   "metadata": {},
   "source": [
    "### 加载模型权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2a8111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果项目路径下没有的话去指定的路径下下载\n",
    "model_path = 'yolov5s.pt'\n",
    "checkpoint_path = attempt_download(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eff068",
   "metadata": {},
   "source": [
    "### 加载yolov5模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a96cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_yolov5 = load_yolov5_model(checkpoint_path, fuse=True)\n",
    "# model_yolov5 = model_yolov5.to(device)\n",
    "# model_yolov5 = model_yolov5.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20671193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_yolov5.names)  # 也可以用 data['names'] 代替"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb5b4b",
   "metadata": {},
   "source": [
    "### 加载yolort模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39916654",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_yolort = YOLO.load_from_yolov5(\n",
    "    checkpoint_path,\n",
    "    score_thresh=score_thresh,\n",
    "    nms_thresh=nms_thresh,\n",
    "    version=\"r6.0\",\n",
    ")\n",
    "model_yolort = model_yolort.eval()\n",
    "model_yolort = model_yolort.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5dbc75",
   "metadata": {},
   "source": [
    "### batch == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a990e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(426, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(428, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(425, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(640, 481, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(478, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(500, 381, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(488, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 480, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(426, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(640, 427, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(640, 565, 3)\n",
      "torch.Size([3, 640, 576])\n",
      "(426, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(375, 500, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(612, 612, 3)\n",
      "torch.Size([3, 640, 640])\n",
      "(425, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(640, 512, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(416, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 416, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(481, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(374, 500, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(573, 640, 3)\n",
      "torch.Size([3, 576, 640])\n",
      "(640, 480, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(500, 600, 3)\n",
      "torch.Size([3, 576, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(428, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(640, 480, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(640, 427, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(536, 640, 3)\n",
      "torch.Size([3, 576, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(428, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 480, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(424, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(333, 500, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(640, 591, 3)\n",
      "torch.Size([3, 640, 640])\n",
      "(640, 428, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(426, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(600, 600, 3)\n",
      "torch.Size([3, 640, 640])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(481, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(491, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(327, 640, 3)\n",
      "torch.Size([3, 384, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(218, 640, 3)\n",
      "torch.Size([3, 256, 640])\n",
      "(332, 500, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(375, 500, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 480, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(640, 480, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 446, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(611, 640, 3)\n",
      "torch.Size([3, 640, 640])\n",
      "(580, 640, 3)\n",
      "torch.Size([3, 640, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 638, 3)\n",
      "torch.Size([3, 640, 640])\n",
      "(640, 426, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(640, 359, 3)\n",
      "torch.Size([3, 640, 384])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(360, 640, 3)\n",
      "torch.Size([3, 384, 640])\n",
      "(389, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(640, 427, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 480, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 516, 3)\n",
      "torch.Size([3, 640, 576])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(226, 640, 3)\n",
      "torch.Size([3, 256, 640])\n",
      "(500, 333, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(406, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(495, 500, 3)\n",
      "torch.Size([3, 640, 640])\n",
      "(313, 500, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 360, 3)\n",
      "torch.Size([3, 640, 384])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 427, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(336, 448, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(425, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(484, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(312, 460, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(640, 423, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(640, 520, 3)\n",
      "torch.Size([3, 640, 576])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 427, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(513, 640, 3)\n",
      "torch.Size([3, 576, 640])\n",
      "(500, 473, 3)\n",
      "torch.Size([3, 640, 640])\n",
      "(426, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(500, 409, 3)\n",
      "torch.Size([3, 640, 576])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(248, 640, 3)\n",
      "torch.Size([3, 256, 640])\n",
      "(407, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(640, 480, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(500, 375, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(446, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(480, 640, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(640, 427, 3)\n",
      "torch.Size([3, 640, 448])\n",
      "(640, 480, 3)\n",
      "torch.Size([3, 640, 512])\n",
      "(428, 640, 3)\n",
      "torch.Size([3, 448, 640])\n",
      "(375, 500, 3)\n",
      "torch.Size([3, 512, 640])\n",
      "(427, 640, 3)\n",
      "torch.Size([3, 448, 640])\n"
     ]
    }
   ],
   "source": [
    "for i, im in enumerate(dataset):\n",
    "    im_path = im[2]\n",
    "    img_raw = cv2.imread(im_path)  # opencv set the BGR order as the default\n",
    "    print(img_raw.shape)\n",
    "    assert img_raw is not None, f\"Not Found Image: {im_path}\"\n",
    "    im = letterbox(img_raw, new_shape=(img_size, img_size), stride=stride)[0]\n",
    "    im = read_image_to_tensor(im)\n",
    "    print(im.size())\n",
    "    im = im.to(device)\n",
    "    im = im[None]\n",
    "#     print(im.size())\n",
    "#     with torch.no_grad():\n",
    "#         yolort_dets = model_yolort(im)\n",
    "#         scale_coords(im.shape[2:], yolort_dets[0]['boxes'], img_raw.shape[:-1])\n",
    "#         v = Visualizer(img_raw, data['names'])\n",
    "#         # Prepare the prediction labels for the Visualizer\n",
    "#         v.draw_instance_predictions(yolort_dets[0])\n",
    "#         v.imshow(scale=0.5)\n",
    "#         print(len(yolort_dets))\n",
    "#         print(img_raw.shape[:-1])  # 原始图片的size\n",
    "#         print(im.shape[2:])\n",
    "#         print(f\"Detection results of image: {i}\")\n",
    "#         print((type(yolort_dets[0])))# 检测结果的类型是dict\n",
    "#         for key in yolort_dets[0]: \n",
    "#             print(key) #用这种方式可以查看到字典中的key值有哪些\n",
    "#             print(yolort_dets[0][key])\n",
    "#             print(yolort_dets[0][key].size())\n",
    "#         print(f\"Detection boxes with yolort:\\n{yolort_dets[0]['boxes']}\")\n",
    "#         print(f\"Detection scores with yolort:\\n{yolort_dets[0]['scores']}\")\n",
    "#         print(f\"Detection labels with yolort:\\n{yolort_dets[0]['labels']}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a9455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b217b68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95:   0%|          | 0/4 [00:00<?, ?it/s]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 640, 640])\n",
      "<class 'numpy.ndarray'>\n",
      "(32, 3, 640, 640)\n",
      "(32, 3, 3)\n",
      "torch.Size([3, 640, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [55]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     24\u001B[0m     im \u001B[38;5;241m=\u001B[39m im\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     25\u001B[0m     targets \u001B[38;5;241m=\u001B[39m targets\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 26\u001B[0m yolort_dets \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_yolort\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch-lsf/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/LISHUFEI/jupyter_code/yolov5-rt-stack/yolort/models/yolo.py:159\u001B[0m, in \u001B[0;36mYOLO.forward\u001B[0;34m(self, samples, targets)\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;124;03m    samples (NestedTensor): Expects a NestedTensor, which consists of:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;124;03m        like `scores`, `labels` and `mask` (for Mask R-CNN models).\u001B[39;00m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;66;03m# get the features from the backbone\u001B[39;00m\n\u001B[0;32m--> 159\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackbone\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;66;03m# compute the yolo heads outputs using the features\u001B[39;00m\n\u001B[1;32m    162\u001B[0m head_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead(features)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch-lsf/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/LISHUFEI/jupyter_code/yolov5-rt-stack/yolort/models/backbone_utils.py:55\u001B[0m, in \u001B[0;36mBackboneWithPAN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 55\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpan(x)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch-lsf/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch-lsf/lib/python3.9/site-packages/torchvision/models/_utils.py:69\u001B[0m, in \u001B[0;36mIntermediateLayerGetter.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     67\u001B[0m out \u001B[38;5;241m=\u001B[39m OrderedDict()\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m---> 69\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_layers:\n\u001B[1;32m     71\u001B[0m         out_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_layers[name]\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch-lsf/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/LISHUFEI/jupyter_code/yolov5-rt-stack/yolort/v5/models/common.py:70\u001B[0m, in \u001B[0;36mConv.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch-lsf/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch-lsf/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:135\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_input_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;66;03m# exponential_average_factor is set to self.momentum\u001B[39;00m\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;66;03m# (when it is available) only so that it gets updated\u001B[39;00m\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001B[39;00m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmomentum \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch-lsf/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:407\u001B[0m, in \u001B[0;36mBatchNorm2d._check_input_dim\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_input_dim\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[0;32m--> 407\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected 4D input (got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124mD input)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()))\n",
      "\u001B[0;31mValueError\u001B[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "\n",
    "s =('%20s' + '%11s' * 6) %  ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "# print(s)\n",
    "pbar = tqdm(loader, desc=s, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\n",
    "# print(type(pbar))\n",
    "\n",
    "for batch_i, (im, targets, paths, shapes) in enumerate(pbar):\n",
    "    print(type(im)) # Tensor 类\n",
    "    print(im.size())  # N ,C, H, W\n",
    "    im = np.ascontiguousarray(im)\n",
    "    print(type(im)) # numpy.ndarray 类\n",
    "    print(im.shape)  # N ,C, H, W\n",
    "#     im = im.transpose((0,3,1,2))\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) # cv2只能处理GBR格式图片\n",
    "    print(im.shape)  # 从这开始和想要的不一样了\n",
    "    im = letterbox(im, new_shape=(img_size, img_size), stride=stride)[0]  # lettebox需要的图像是np.ndarray类型的\n",
    "#     im = Image.fromarray(cv2.cvtColor(im,cv2.COLOR_BGR2RGB))\n",
    "    if cuda:\n",
    "#         print(type(im)) # 每个 和 paths 中都存储了32张图片的信息，\n",
    "#         print(len(im))\n",
    "#         print(len(paths)) \n",
    "#         print(type(paths)) # path 是 list 类型的， 长度为32\n",
    "        im = read_image_to_tensor(im) \n",
    "        print(im.size())\n",
    "        im = im.to(device)\n",
    "        targets = targets.to(device)\n",
    "    yolort_dets = model_yolort(im)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17a467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73d75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}